{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Dataframes: \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concise: \n",
    "Unique ID | Listing Price | Asset creation date | Sale start | Sale duration | Primary Sale (boolean) | Number of Sales | Buy or Sale | Sale type | Creator Fee\n",
    "\n",
    "Raw:\n",
    "Everything\n",
    "\n",
    "Responses:\n",
    "Get responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook\n",
    "1. Read orderbook_samples.csv, set listed_before variable to earliest timestamp. If no timestamp found, \n",
    "set listed_before to current time.\n",
    "2. Call Orderbook API X number of times, get Y orders before listed_before.\n",
    "3. Produce target dataframes. \n",
    "4. Append new data to orderbook_samples.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp(file_name, time_step=86400):\n",
    "    #get unix timestamp for the time of the earliest transaction in csv minus time_step.\n",
    "    # time_step is one day by default.\n",
    "    # If file doesn't exit, gets current timestamp.\n",
    "\n",
    "    t0 = pd.Timestamp(\"1970-01-01\")\n",
    "    if os.path.exists(file_name):\n",
    "        csv = pd.read_csv(file_name, index_col=False)\n",
    "        data_time = pd.to_datetime(csv.tail(1)['created_date'])\n",
    "        timestamps = data_time.apply(datetime.timestamp)\n",
    "\n",
    "        return timestamps.values[-1] - time_step\n",
    "    else:\n",
    "        return datetime.timestamp(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orders(listed_before, query_limit = 20, order_limit=50):\n",
    "    #calls OpenSea orderbook API, creates a dataframe of length order_limit * query_limit\n",
    "    df = pd.DataFrame()\n",
    "    df_simple = pd.DataFrame()\n",
    "    q = 0\n",
    "    QUERY_COUNT = 0\n",
    "    while True:\n",
    "        url = f\"https://api.opensea.io/wyvern/v1/orders?bundled=false&include_bundled=false&include_invalid=false&listed_before={listed_before}&limit={order_limit}&offset={order_limit*q}&order_by=created_date&order_direction=desc\"\n",
    "        headers = {\"Accept\": \"application/json\"}\n",
    "        response = requests.request(\"GET\", url, headers=headers)\n",
    "        if response.ok is False:\n",
    "            print('API call failed')\n",
    "            return df, df_simple\n",
    "        df = df.append(response_to_df(response),\n",
    "                       ignore_index=True)\n",
    "        df.sort_values('id', ascending = False,inplace=True)\n",
    "        df_simple = drop_columns(df.copy())\n",
    "        QUERY_COUNT += 1\n",
    "        q += 1\n",
    "        if QUERY_COUNT >= query_limit:\n",
    "            # print(\"Query completed!\")\n",
    "            return df, df_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIMIT = 50\n",
    "# listed_before = 1635621155\n",
    "# url = f\"https://api.opensea.io/wyvern/v1/orders?bundled=false&include_bundled=false&include_invalid=false&listed_before={listed_before}&limit={LIMIT}&offset=0&order_by=created_date&order_direction=desc\"\n",
    "# headers = {\"Accept\": \"application/json\"}\n",
    "# response = requests.request(\"GET\", url, headers=headers)\n",
    "# df = response_to_df(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_to_df(response):\n",
    "    # Clean up the response.json() a bit, and extracts some information from dictionary columns\n",
    "    # Runs into error if value is none in column\n",
    "    df = pd.DataFrame(response.json()['orders'])\n",
    "    #drops rows that contain bundles\n",
    "    df['bundle'] = df['metadata'].apply(lambda x: 1\n",
    "                                      if 'bundle' in x.keys() else 0)\n",
    "    df = df[df['bundle'] == 0]\n",
    "    #creates unique id for each asset\n",
    "    asset_unique_id = pd.DataFrame(pd.DataFrame(\n",
    "        df[\"metadata\"].tolist())['asset'].tolist())\n",
    "    df['asset_unique_id'] = asset_unique_id[['id', 'address']].agg('-'.join,\n",
    "                                                               axis=1)\n",
    "    #extracts information from asset dictionary\n",
    "    df['number_sales'] = pd.DataFrame(df[\"asset\"].tolist())['num_sales']\n",
    "    df['creator_fee'] = pd.DataFrame(\n",
    "        pd.DataFrame(df[\"asset\"].tolist())\n",
    "        ['asset_contract'].tolist())['seller_fee_basis_points']\n",
    "    df['description'] = pd.DataFrame(df[\"asset\"].tolist())['description']\n",
    "    df['external_link'] = pd.DataFrame(df[\"asset\"].tolist())['external_link']\n",
    "    df['image_url'] = pd.DataFrame(df[\"asset\"].tolist())['image_url']\n",
    "    df['animation_url'] = pd.DataFrame(df[\"asset\"].tolist())['animation_url']\n",
    "    df['sale_duration'] = pd.to_datetime(df['closing_date']) - pd.to_datetime(df['created_date'])\n",
    "    #drops asset column\n",
    "    df.drop(columns=\"asset\", inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(\n",
    "    df,\n",
    "    delete_columns=[\n",
    "        'expiration_time', 'listing_time', 'current_bounty', 'bounty_multiple',\n",
    "        'maker_relayer_fee', 'taker_relayer_fee', 'maker_protocol_fee',\n",
    "        'taker_protocol_fee', 'maker_referrer_fee', 'fee_recipient',\n",
    "        'fee_method', 'target', 'how_to_call', 'calldata',\n",
    "        'replacement_pattern', 'static_target', 'static_extradata', 'extra',\n",
    "        'quantity', 'salt', 'v', 'r', 's', 'approved_on_chain', 'cancelled',\n",
    "        'finalized', 'asset_bundle', 'closing_extendable', 'order_hash',\n",
    "        'metadata', 'exchange', 'maker', 'taker', 'marked_invalid',\n",
    "        'prefixed_hash', 'bundle', 'asset_unique_id', 'number_sales'\n",
    "    ]):\n",
    "    #leave only essential columns\n",
    "    for i in delete_columns:\n",
    "        df.drop(columns=i, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(df, file_name, append=True):\n",
    "    # Save df to a .csv, supports appending\n",
    "    if append and os.path.exists(file_name):\n",
    "        df_read = pd.read_csv(file_name, index_col=False)\n",
    "        df_read = df_read.append(df, ignore_index=True)\n",
    "        #df_read.drop_duplicates(keep=\"first\", inplace=True) # Doesn't work for some reason! also breaks with dict\n",
    "        df_read.to_csv(file_name, index=False)\n",
    "    else:\n",
    "        df.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar\n",
    "\n",
    "def main(iterations,\n",
    "         df_file_name='orderbook_samples.csv',\n",
    "         simple_df_file_name='orderbook_samples_essential.csv'):\n",
    "    # run this to add rows to csv\n",
    "    for i in progressbar.progressbar(range(iterations)):\n",
    "        listed_before_timestamp = get_timestamp(df_file_name)\n",
    "        df,df_simple = get_orders(listed_before_timestamp)\n",
    "        save_df(df, df_file_name)\n",
    "        save_df(df_simple, simple_df_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It takes 20-40 seconds to add 1000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2 of 2) |##########################| Elapsed Time: 0:01:36 Time:  0:01:36\n"
     ]
    }
   ],
   "source": [
    "main(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3998, 53)\n",
      "(3998, 27)\n"
     ]
    }
   ],
   "source": [
    "# check csv length is correctly updated. The number of rows added may be\n",
    "#   slightly less than what you expect because bundle orders are dropped.\n",
    "print(pd.read_csv('orderbook_samples.csv').shape)\n",
    "print(pd.read_csv('orderbook_samples_essential.csv').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'asset_bundle', 'created_date', 'closing_date',\n",
       "       'closing_extendable', 'order_hash', 'metadata', 'exchange', 'maker',\n",
       "       'taker', 'current_price', 'side', 'sale_kind', 'payment_token',\n",
       "       'payment_token_contract', 'base_price', 'marked_invalid',\n",
       "       'prefixed_hash', 'bundle', 'asset_unique_id', 'number_sales',\n",
       "       'creator_fee', 'description', 'external_link', 'image_url',\n",
       "       'animation_url', 'sale_duration'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('orderbook_samples_essential.csv').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "   'asset_bundle',  'closing_extendable',\n",
    "    'order_hash', 'metadata', 'exchange', 'maker', 'taker', 'marked_invalid', 'prefixed_hash', 'bundle',\n",
    "    'asset_unique_id', 'number_sales'\n",
    "]\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79ee92f60ffe45c2eaed0e231ff59efcb2fec85c2d5b8c1f60acb0bd5501f65c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('lewagon': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
