{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Dataframes: \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concise: \n",
    "Unique ID | Listing Price | Asset creation date | Sale start | Sale duration | Primary Sale (boolean) | Number of Sales | Buy or Sale | Sale type | Creator Fee\n",
    "\n",
    "Raw:\n",
    "Everything\n",
    "\n",
    "Responses:\n",
    "Get responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook\n",
    "1. Read orderbook_samples.csv, set listed_before variable to earliest timestamp. If no timestamp found, \n",
    "set listed_before to current time.\n",
    "2. Call Orderbook API X number of times, get Y orders before listed_before.\n",
    "3. Produce target dataframes. \n",
    "4. Append new data to orderbook_samples.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp(file_name, time_step=86400):\n",
    "    #get unix timestamp for the created date in the last row of a file.\n",
    "    # If file doesn't exit, gets current timestamp\n",
    "    t0 = pd.Timestamp(\"1970-01-01\")\n",
    "    if os.path.exists(file_name):\n",
    "        csv = pd.read_csv(file_name, index_col=False)\n",
    "        data_time = pd.to_datetime(csv['created_date'])\n",
    "        timestamps = data_time.apply(datetime.timestamp)\n",
    "\n",
    "        return timestamps.values[-1] - time_step\n",
    "    else:\n",
    "        return datetime.timestamp(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orders(listed_before, query_limit = 20, order_limit=50):\n",
    "    #calls OpenSea orderbook API, creates a dataframe of length order_limit * query_limit\n",
    "    df = pd.DataFrame()\n",
    "    q = 0\n",
    "    QUERY_COUNT = 0\n",
    "    while True:\n",
    "        url = f\"https://api.opensea.io/wyvern/v1/orders?bundled=false&include_bundled=false&include_invalid=false&listed_before={listed_before}&limit={order_limit}&offset={order_limit*q}&order_by=created_date&order_direction=desc\"\n",
    "        headers = {\"Accept\": \"application/json\"}\n",
    "        response = requests.request(\"GET\", url, headers=headers)\n",
    "        if response.ok is False:\n",
    "            print('API call failed')\n",
    "            break\n",
    "        df = df.append(response_to_df(response),\n",
    "                       ignore_index=True)\n",
    "        df.sort_values('id', ascending = False,inplace=True)\n",
    "        QUERY_COUNT += 1\n",
    "        q += 1\n",
    "        if QUERY_COUNT >= query_limit:\n",
    "            print(\"Query count reached!\")\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIMIT = 10\n",
    "# listed_before = 1635621155\n",
    "# url = f\"https://api.opensea.io/wyvern/v1/orders?bundled=false&include_bundled=false&include_invalid=false&listed_before={listed_before}&limit={LIMIT}&offset=0&order_by=created_date&order_direction=desc\"\n",
    "# headers = {\"Accept\": \"application/json\"}\n",
    "# response = requests.request(\"GET\", url, headers=headers)\n",
    "# df = response_to_df(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_to_df(response):\n",
    "    # Clean up the response.json() a bit, and extracts some information from dictionary columns\n",
    "    # Runs into error if value is none in column\n",
    "    df = pd.DataFrame(response.json()['orders'])\n",
    "    df['bundle'] = df['metadata'].apply(lambda x: 1\n",
    "                                      if 'bundle' in x.keys() else 0)\n",
    "    df = df[df['bundle'] == 0]\n",
    "    asset_unique_id = pd.DataFrame(pd.DataFrame(\n",
    "        df[\"metadata\"].tolist())['asset'].tolist())\n",
    "\n",
    "    df['asset_unique_id'] = asset_unique_id[['id', 'address']].agg('-'.join,\n",
    "                                                               axis=1)\n",
    "    df['number_sales'] = pd.DataFrame(df[\"asset\"].tolist())['num_sales']\n",
    "    df['creator_fee'] = pd.DataFrame(\n",
    "        pd.DataFrame(df[\"asset\"].tolist())\n",
    "        ['asset_contract'].tolist())['seller_fee_basis_points']\n",
    "    df['description'] = pd.DataFrame(df[\"asset\"].tolist())['description']\n",
    "    df['external_link'] = pd.DataFrame(df[\"asset\"].tolist())['external_link']\n",
    "    df['image_url'] = pd.DataFrame(df[\"asset\"].tolist())['image_url']\n",
    "    df['animation_url'] = pd.DataFrame(df[\"asset\"].tolist())['animation_url']\n",
    "    df.drop(columns=\"asset\", inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(df, file_name, append=True):\n",
    "    # Save df to a .csv, supports appending\n",
    "    if append and os.path.exists(file_name):\n",
    "        df_read = pd.read_csv(file_name, index_col=False)\n",
    "        df_read = df_read.append(df, ignore_index=True)\n",
    "        #df_read.drop_duplicates(keep=\"first\", inplace=True) # Doesn't work for some reason! also breaks with dict\n",
    "        df_read.to_csv(file_name, index=False)\n",
    "    else:\n",
    "        df.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query count reached!\n"
     ]
    }
   ],
   "source": [
    "file_name = 'orderbook_samples.csv'\n",
    "listed_before_timestamp = get_timestamp(file_name)\n",
    "df = get_orders(listed_before_timestamp)\n",
    "save_df(df,file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It takes 20-40 seconds to add 1000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2996, 52)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('orderbook_samples.csv').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79ee92f60ffe45c2eaed0e231ff59efcb2fec85c2d5b8c1f60acb0bd5501f65c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('lewagon': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
